from carculator import *
import xarray as xr
import numpy as np
import json
import os

class LCAModel():

    def __init__(self, car_config):
        self.config = car_config
        self.GENERAL = "GENERAL"

    ###
    # Generates data for the use in the main pipeline
    # The data generated by this method has the following format:
    # { parameters: [ {name, type, index} ] list of size N containing name and type (numerical, nominal)
    # of the different parameters
    #   data: {
    #           X: np.array of size NxM containing the parameter values
    #           y: { impact_category: [] np.array of size 1xM } containing the impact results for
    #           the corresponding impact category
    #         }
    # }
    ###

    def generate_data(self, min_data_set_size):
        self.cip = CarInputParameters()

        parameters = self.cip.input_parameters.copy()
        nominal_parameters = np.zeros(len(parameters))

        self.countries = dict(self.config.items('countries')).values()
        self.driving_cycles = dict(self.config.items('countries')).values()

        if self.countries:
            parameters.append("country")

        parameters.append("size")
        parameters.append("powertrain")
        parameters.append("year")

        nominals = np.ones(len(parameters) - len(nominal_parameters))
        # change year to non-nominal
        nominals[-1] = 0

        self.data_set = {}
        self.data_set["parameter_names"] = parameters
        self.data_set["nominal_parameters"] = list(np.concatenate((nominal_parameters, nominals)))


        #Calculate the number of iterations required to reach min_data_set_size
        num_countries = len(self.countries)
        if num_countries < 1:
            print("At least one country needs to be specified for data generation. Aborting.")
            return

        num_driving_cycles = len(self.driving_cycles)

        num_iterations = int( min_data_set_size/num_countries )

        self.cip.stochastic(num_iterations)
        dcts, array = fill_xarray_from_input_parameters(self.cip)
        cm = CarModel(array)
        cm.set_all()
        params_array = cm.array.sel(parameter=self.cip.input_parameters)

        results = []

        # after calculations, results have the following properties:
        # results array with entry for each country
        # each result: {
        #   'impact category'
        #   'size'
        #   'powertrain'
        #   'year'
        #   'impact' will likely be summed up
        #   'value'
        # }


        results_by_impact = {}
        all_params = []

        for country in self.countries:
            bc = {'country': country}
            ic = InventoryCalculation(cm.array, background_configuration=bc)
            result = ic.calculate_impacts()

            # Sum impact attributions (only use summed impact for every category)
            if not self.config[self.GENERAL].getboolean("retain_attributions"):
                result = result.sum("impact")
            else:
                self.data_set["attribution_names"] = result.coords["impact"].values

            for size in result.coords["size"].values:
                for powertrain in result.coords["powertrain"].values:

                    # Ignore plug in hybrid vehicles. (should not be in since they are not calculated)
                    if not self.config[self.GENERAL]["use_plug_in_hybrids"]:
                        if powertrain in ['PHEV-p', 'PHEV-d']:
                            continue

                    for year in result.coords["year"].values:
                        additional_params = np.array([country, size, powertrain, year])
                        for value in result.coords["value"].values:
                            params = params_array.sel({"size": size, "powertrain": powertrain, "year": year, "value": value})
                            all_params.append(np.concatenate((np.array(params),additional_params)))
                            for impact_category in result.coords["impact_category"].values:
                                if not impact_category in results_by_impact:
                                    results_by_impact[impact_category] = []
                                impact = result.sel({"impact_category": impact_category, "size": size, "powertrain": powertrain, "year": year, "value": value})
                                results_by_impact[impact_category].append(float(impact))

        self.data_set["y"] = {}
        for impact_category in results_by_impact:
            self.data_set["y"][impact_category] = np.vstack(results_by_impact[impact_category])

        self.data_set["X"] = np.vstack(all_params)

    # two files are saved, one is the meta file containing all the naming information the second
    # holds the raw data X and y
    def save_data(self,path_to_data, num_files=-1):

        try:
            os.makedirs(path_to_data)
        except FileExistsError:
            print("Saving directory already exists, therefore existing files are potentially overwritten")
            pass

        data_set = self.data_set.copy()
        X = data_set.pop("X")
        y = data_set.pop("y")

        with open(os.path.join(path_to_data, "meta_information.json"), "w") as f:
            json.dump(data_set, f)

        if num_files > 0:
            #split data into several arrays
            num_data_per_file = int(X.shape[0]/num_files)
            for i in range(num_files):
                new_y = {}
                for cat in y:
                    new_y[cat] = y[cat][i*num_data_per_file:(i+1)*num_data_per_file]

                np.savez_compressed(os.path.join(path_to_data, "X_chunk{}.npz".format(i)), X=X[i*num_data_per_file:(i+1)*num_data_per_file])
                np.savez_compressed(os.path.join(path_to_data, "y_chunk{}.npz".format(i)), **new_y)
        else:
            np.savez_compressed(os.path.join(path_to_data, "X.npz"), X=X)
            np.savez_compressed(os.path.join(path_to_data, "y.npz"), **y)

